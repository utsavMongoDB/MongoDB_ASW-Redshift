{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# MongoDB To RedShift Data Transfer \n\nThis notebook demostrates, how we can use direct MongoDB and Redshift connectors to move data. ",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "### Pulling data from MongoDB\nThe below function uses MongoDB data connection (configured udner data connections). ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\n\nsc = SparkContext.getOrCreate();\nglueContext = GlueContext(sc)\n\nmongodb_read = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"mongodb\",\n    connection_options={\n        \"connectionName\": \"Mongodbatlas connection\",\n        \"database\": \"big_store\",\n        \"collection\": \"orders\",\n        \"partitioner\": \"com.mongodb.spark.sql.connector.read.partitioner.SinglePartitionPartitioner\",\n        \"partitionerOptions.partitionSizeMB\": \"10\",\n        \"partitionerOptions.partitionKey\": \"_id\",\n        \"disableUpdateUri\": \"false\",\n    }\n)",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Printing the MongoDB Schema",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "mongodb_read.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Printing data as Dataframe\n\nThe data shown can be translated into dataframe to have a look at the data coming in.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "mongodb_read.toDF().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Writing data to Redshift\nThe data can be finally written to Redshift using the data connectors configured. ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "redshift_options = {\n    \"dbtable\": \"public.orders\",  # Destination table name in Redshift\n    \"user\": \"admin\",\n    \"password\": \"Test#123\",\n    \"database\" : \"dev\",\n    \"connectTimeout\": 10000\n\n}\n\nglueContext.write_dynamic_frame.from_jdbc_conf(\n    frame=mongodb_read,\n    catalog_connection=\"Redshift connection\",  # Glue Data Catalog connection name for Redshift\n    connection_options=redshift_options,\n    redshift_tmp_dir=\"s3://utsav-demo-partner/\"  # S3 path for temporary files (required)\n)\n\nprint(\"Data Written to Redshift!\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": []
		}
	]
}